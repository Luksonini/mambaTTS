#!/usr/bin/env python3
"""
NAPRAWIONY Stateful vs Stateless Pure Mamba Comparison Test
==========================================================
Poprawki:
1. Usuniƒôto b≈Çƒôdy sk≈Çadniowe
2. Zunifikowano gradient clipping
3. Poprawiono zarzƒÖdzanie stanami
4. Dodano monitoring fallback√≥w
5. LepszƒÖ obs≈Çugƒô b≈Çƒôd√≥w
6. PROPER stateful processing - ka≈ºdy sample ma w≈Çasny persistent state
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import logging
import warnings
import json
import time
import copy
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union
from dataclasses import dataclass, asdict
import matplotlib.pyplot as plt

def get_mamba_state(model) -> List[Tuple[torch.Tensor, torch.Tensor]]:
    """
    Zbiera ukryte stany (_hidden_state i _conv_state) ze wszystkich StatefulMambaBlock
    w modelu, w kolejno≈õci:
      - text_encoder.layers
      - audio_processor.mamba_layers

    Zwraca listƒô krotek (hidden_state, conv_state).
    """
    states = []
    # text encoder
    for layer in model.text_encoder.layers:
        if hasattr(layer, '_hidden_state'):
            states.append((
                layer._hidden_state.clone(),
                layer._conv_state.clone() if layer._conv_state is not None else None
            ))
    # audio processor
    for layer in model.audio_processor.mamba_layers:
        states.append((
            layer._hidden_state.clone(),
            layer._conv_state.clone() if layer._conv_state is not None else None
        ))
    return states


def set_mamba_state(model, states: List[Tuple[torch.Tensor, torch.Tensor]]):
    """
    Ustawia ukryte stany (_hidden_state i _conv_state) dla wszystkich StatefulMambaBlock
    z listy states, w tej samej kolejno≈õci co get_mamba_state.
    """
    idx = 0
    # text encoder
    for layer in model.text_encoder.layers:
        if hasattr(layer, '_hidden_state'):
            hidden, conv = states[idx]
            layer._hidden_state = hidden.detach()
            layer._conv_state = conv.detach() if conv is not None else None
            idx += 1
    # audio processor
    for layer in model.audio_processor.mamba_layers:
        hidden, conv = states[idx]
        layer._hidden_state = hidden.detach()
        layer._conv_state = conv.detach() if conv is not None else None
        idx += 1


warnings.filterwarnings("ignore", message=".*weight_norm.*deprecated.*")
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

print("üî¨ NAPRAWIONY STATEFUL vs STATELESS PURE MAMBA COMPARISON")
print("=" * 60)

# Import existing components
try:
    from nucleotide_tokenizer import NucleotideTokenizer
    from losses import compute_combined_loss
    logger.info("‚úÖ Imported tokenizer and losses")
except ImportError as e:
    logger.error(f"‚ùå Import error: {e}")
    exit(1)


@dataclass
class StatefulComparisonConfig:
    """Configuration for stateful comparison experiments"""
    name: str
    num_layers: int = 4
    hidden_dim: int = 768
    expand_factor: float = 1.5  # Optimal from research
    embed_dim: int = 384
    num_codebooks: int = 8
    codebook_size: int = 1024
    dropout: float = 0.1
    stateful: bool = False
    
    def get_param_count_estimate(self) -> int:
        """Estimate parameter count - should be ~64.8M for optimal config"""
        vocab_embed = 1000 * self.embed_dim
        text_encoder = self.num_layers * (self.hidden_dim * self.hidden_dim * 4)
        audio_processor = 4 * (self.hidden_dim * self.hidden_dim * 3)
        others = self.hidden_dim * 1000
        return vocab_embed + text_encoder + audio_processor + others


class StatefulMambaBlock(nn.Module):
    """
    NAPRAWIONY Pure Mamba block with optional state persistence
    """
    def __init__(self, d_model, expand_factor=1.5, dropout=0.1, reverse=False):
        super().__init__()
        self.d_model = d_model
        self.d_inner = int(d_model * expand_factor)
        self.reverse = reverse
        
        # Core Mamba components (optimized L4_H768_E1.5 design)
        self.in_proj = nn.Linear(d_model, self.d_inner * 2, bias=False)
        
        # Efficient depthwise convolution
        self.conv1d = nn.Conv1d(
            self.d_inner, 
            self.d_inner, 
            kernel_size=3, 
            padding=1, 
            groups=self.d_inner,
            bias=False
        )
        
        # Optimized SSM parameters
        self.x_proj = nn.Linear(self.d_inner, self.d_inner, bias=False)
        self.dt_proj = nn.Linear(self.d_inner, self.d_inner, bias=True)
        
        # Output projection with layer scaling
        self.out_proj = nn.Linear(self.d_inner, d_model, bias=False)
        self.layer_scale = nn.Parameter(torch.ones(d_model) * 0.1)
        
        # Efficient normalization
        self.norm = nn.LayerNorm(d_model)
        self.activation = nn.SiLU()
        self.dropout = nn.Dropout(dropout)
        
        # NAPRAWIONE STATE MANAGEMENT
        self.stateful_mode = False
        self._hidden_state = None
        self._conv_state = None
        
    def enable_stateful_mode(self, batch_size=1):
        """Enable state persistence across chunks"""
        self.stateful_mode = True
        device = next(self.parameters()).device
        # Initialize states for batch
        self._hidden_state = torch.zeros(batch_size, 1, self.d_inner, device=device)
        self._conv_state = torch.zeros(batch_size, self.d_inner, 2, device=device)
    
    def disable_stateful_mode(self):
        """Disable state persistence - fresh state each chunk"""
        self.stateful_mode = False
        self._hidden_state = None
        self._conv_state = None
    
    def reset_state(self):
        """Reset hidden states (for new batch)"""
        if self.stateful_mode and self._hidden_state is not None:
            self._hidden_state.zero_()
            if self._conv_state is not None:
                self._conv_state.zero_()
    
    def forward(self, x, return_state_info=False):
        B, L, D = x.shape
        residual = x
        
        # Pre-normalization
        x = self.norm(x)
        
        # Reverse for backward processing
        if self.reverse:
            x = torch.flip(x, dims=[1])
        
        # Input projection and split
        x_proj = self.in_proj(x)  # [B, L, 2*d_inner]
        x1, x2 = x_proj.chunk(2, dim=-1)  # Each [B, L, d_inner]
        
        # UPROSZCZONA KONWOLUCJA (bez stateful - powodowa≈Ça problemy)
        x1_conv = self.conv1d(x1.transpose(1, 2)).transpose(1, 2)
        
        # SIMPLIFIED STATEFUL SSM - bez wolnej pƒôtli!
        x1_ssm = self.activation(x1_conv)
        dt = self.dt_proj(x1_ssm)
        dt = F.softplus(dt)
        
        # SZYBKA WERSJA: Zamiast _stateful_ssm, u≈ºyj prostej approximacji
        if self.stateful_mode and self._hidden_state is not None:
            alpha = 0.1
            # standardowy update dla ca≈Çej sekwencji
            x1_processed = x1_ssm * torch.sigmoid(dt)
            # inject stan tylko do pierwszej pozycji
            x1_processed[:, 0, :] = x1_processed[:, 0, :] \
                + alpha * self._hidden_state.squeeze(1)
            # zapamiƒôtaj nowy stan z ko≈Ñca sekwencji
            self._hidden_state = x1_processed[:, -1:, :].detach()
        else:
            x1_processed = x1_ssm * torch.sigmoid(dt)
        
        # Gating mechanism
        x_gated = x1_processed * torch.sigmoid(x2)
        
        # Output projection with layer scaling
        output = self.out_proj(x_gated)
        output = output * self.layer_scale
        
        # Dropout and reverse back
        output = self.dropout(output)
        if self.reverse:
            output = torch.flip(output, dims=[1])
        
        final_output = output + residual
        
        if return_state_info:
            state_info = {
                'has_hidden_state': self.stateful_mode and self._hidden_state is not None,
                'hidden_state_norm': torch.norm(self._hidden_state).item() if self._hidden_state is not None else 0.0,
                'stateful_mode': self.stateful_mode
            }
            return final_output, state_info
        
        return final_output


class StatefulMambaTextEncoder(nn.Module):
    """Text encoder with optional state persistence across chunks"""
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, expand_factor, dropout=0.1):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.embed_proj = nn.Linear(embed_dim, hidden_dim) if embed_dim != hidden_dim else nn.Identity()
        
        # Learnable positional encoding
        self.pos_encoding = nn.Parameter(torch.randn(1, 2048, hidden_dim) * 0.02)
        
        # Stack of stateful Mamba blocks
        self.layers = nn.ModuleList([
            StatefulMambaBlock(hidden_dim, expand_factor, dropout, reverse=False)
            for _ in range(num_layers)
        ])
        
        # Final processing
        self.norm = nn.LayerNorm(hidden_dim)
        self.dropout = nn.Dropout(dropout)
        
    def enable_stateful_mode(self, batch_size=1):
        """Enable state persistence for all layers"""
        for layer in self.layers:
            layer.enable_stateful_mode(batch_size)
    
    def disable_stateful_mode(self):
        """Disable state persistence for all layers"""
        for layer in self.layers:
            layer.disable_stateful_mode()
    
    def reset_states(self):
        """Reset all layer states"""
        for layer in self.layers:
            layer.reset_state()
    
    def forward(self, tokens, return_sequence=True, return_state_info=False):
        B, L = tokens.shape
        
        # Embeddings and projection
        x = self.embedding(tokens)
        x = self.embed_proj(x)
        
        # Add positional encoding
        if L <= self.pos_encoding.shape[1]:
            x = x + self.pos_encoding[:, :L, :]
        
        x = self.dropout(x)
        
        # Process through stateful Mamba layers
        state_infos = []
        for layer in self.layers:
            if return_state_info:
                x, state_info = layer(x, return_state_info=True)
                state_infos.append(state_info)
            else:
                x = layer(x)
        
        x = self.norm(x)
        
        result = x if return_sequence else x.mean(dim=1)
        
        if return_state_info:
            return result, state_infos
        return result


class StatefulMambaAudioProcessor(nn.Module):
    """Audio processor with stateful Mamba blocks"""
    def __init__(self, hidden_dim, num_codebooks=8, codebook_size=1024, expand_factor=1.5):
        super().__init__()
        self.num_codebooks = num_codebooks
        self.codebook_size = codebook_size
        self.hidden_dim = hidden_dim
        
        # Efficient embeddings
        self.audio_embed = nn.ModuleList([
            nn.Sequential(
                nn.Embedding(codebook_size, hidden_dim),
                nn.LayerNorm(hidden_dim),
                nn.Dropout(0.1)
            ) for _ in range(num_codebooks)
        ])
        
        self.context_proj = nn.Linear(hidden_dim, hidden_dim)
        
        # Stateful Mamba processing layers (4 layers for L4 config)
        self.mamba_layers = nn.ModuleList([
            StatefulMambaBlock(hidden_dim, expand_factor, dropout=0.1)
            for _ in range(4)
        ])
        
        # Efficient output heads
        self.output_heads = nn.ModuleList([
            nn.Sequential(
                nn.LayerNorm(hidden_dim),
                nn.Linear(hidden_dim, hidden_dim // 2),
                nn.GELU(),
                nn.Dropout(0.1),
                nn.Linear(hidden_dim // 2, codebook_size)
            ) for _ in range(num_codebooks)
        ])
    
    def enable_stateful_mode(self, batch_size=1):
        """Enable state persistence for audio processing"""
        for layer in self.mamba_layers:
            layer.enable_stateful_mode(batch_size)
    
    def disable_stateful_mode(self):
        """Disable state persistence for audio processing"""
        for layer in self.mamba_layers:
            layer.disable_stateful_mode()
    
    def reset_states(self):
        """Reset all audio processor states"""
        for layer in self.mamba_layers:
            layer.reset_state()
    
    def forward(self, audio_tokens, text_context, return_state_info=False):
        B, C, T = audio_tokens.shape
        
        # Ensure 8 codebooks
        if C < 8:
            padding = torch.zeros(B, 8 - C, T, dtype=audio_tokens.dtype, device=audio_tokens.device)
            audio_tokens = torch.cat([audio_tokens, padding], dim=1)
        elif C > 8:
            audio_tokens = audio_tokens[:, :8, :]
        
        # Embed each codebook
        embedded = []
        for c in range(8):
            emb = self.audio_embed[c][0](audio_tokens[:, c, :])
            emb = self.audio_embed[c][1](emb)
            emb = self.audio_embed[c][2](emb)
            embedded.append(emb)
        
        # Combine embeddings
        x = torch.stack(embedded, dim=1).mean(dim=1)  # [B, T, hidden_dim]
        
        # Add text context
        text_context_proj = self.context_proj(text_context).unsqueeze(1)
        x = x + text_context_proj
        
        # Process through stateful Mamba layers
        state_infos = []
        for layer in self.mamba_layers:
            if return_state_info:
                x, state_info = layer(x, return_state_info=True)
                state_infos.append(state_info)
            else:
                x = layer(x)
        
        # Generate logits for each codebook
        logits = []
        for c in range(8):
            head_logits = self.output_heads[c](x)
            logits.append(head_logits)
        
        result = torch.stack(logits, dim=1)
        
        if return_state_info:
            return result, state_infos
        return result


class StatefulPureMambaModel(nn.Module):
    """
    Pure Mamba model with optional state persistence
    Optimal L4_H768_E1.5 configuration (64.8M parameters)
    """
    def __init__(self, config: StatefulComparisonConfig):
        super().__init__()
        self.config = config
        self.stateful = config.stateful
        
        # All components use Pure Mamba with optimal L4_H768_E1.5 config
        self.text_encoder = StatefulMambaTextEncoder(
            vocab_size=1000,  # Will be updated
            embed_dim=config.embed_dim,
            hidden_dim=config.hidden_dim,
            num_layers=config.num_layers,
            expand_factor=config.expand_factor,
            dropout=config.dropout
        )
        
        # Duration and style components (simplified for focus on core comparison)
        self.duration_regulator = nn.Sequential(
            nn.Linear(config.hidden_dim, config.hidden_dim // 2),
            nn.GELU(),
            nn.Linear(config.hidden_dim // 2, 1),
            nn.Softplus()
        )
        
        self.audio_processor = StatefulMambaAudioProcessor(
            hidden_dim=config.hidden_dim,
            num_codebooks=config.num_codebooks,
            codebook_size=config.codebook_size,
            expand_factor=config.expand_factor
        )
        
        # Projections
        self.text_proj = nn.Sequential(
            nn.Linear(config.hidden_dim, config.hidden_dim),
            nn.LayerNorm(config.hidden_dim),
            nn.GELU(),
            nn.Dropout(config.dropout)
        )
        
        # Log model info
        total_params = sum(p.numel() for p in self.parameters())
        logger.info(f"üß† {config.name}: {total_params:,} parameters")
        logger.info(f"   üìè L{config.num_layers}_H{config.hidden_dim}_E{config.expand_factor}")
        logger.info(f"   üîó Stateful: {config.stateful}")
        
    def enable_stateful_mode(self, batch_size=1):
        """Enable state persistence across all components"""
        if not self.stateful:
            return
        
        self.text_encoder.enable_stateful_mode(batch_size)
        self.audio_processor.enable_stateful_mode(batch_size)
    
    def disable_stateful_mode(self):
        """Disable state persistence across all components"""
        self.text_encoder.disable_stateful_mode()
        self.audio_processor.disable_stateful_mode()
    
    def reset_all_states(self):
        """Reset all model states (call between batches)"""
        if not self.stateful:
            return
        
        self.text_encoder.reset_states()
        self.audio_processor.reset_states()
    
    def forward(self, text_tokens, audio_tokens=None, return_state_info=False):
        batch_size = text_tokens.shape[0]
        
        # Process text
        text_features = self.text_encoder(text_tokens, return_sequence=True)
        text_context = self.text_encoder(text_tokens, return_sequence=False)
        text_context = self.text_proj(text_context)
        
        # Duration prediction (simplified)
        predicted_durations = self.duration_regulator(text_features.mean(dim=1))
        
        # Audio processing
        if audio_tokens is not None:
            regulated_context = text_context
            audio_logits = self.audio_processor(audio_tokens, regulated_context)
        else:
            audio_logits = None
        
        result = {
            'logits': audio_logits,
            'predicted_durations': predicted_durations,
            'text_features': text_features,
        }
        
        return result


class ProperStatefulDataLoader:
    """
    Proper DataLoader for stateful vs stateless comparison
    Each sample (folder) has its own state that persists across chunks
    """
    def __init__(self, data_dir="no_overlap_data", device='cpu', max_samples=4):
        self.data_dir = Path(data_dir)
        self.device = device
        self.samples = {}  # sample_id -> list of chunks (in order)
        self.max_samples = max_samples
        self.current_chunk_idx = 0
        self.max_chunks_per_sample = 0
        
        logger.info(f"üîç Loading proper stateful data from {data_dir}")
        self._load_samples()
        
    def _load_samples(self):
        """Load samples (each folder = one sample with sequential chunks)"""
        if not self.data_dir.exists():
            logger.error(f"‚ùå Data directory not found: {self.data_dir}")
            return
            
        sample_dirs = [d for d in self.data_dir.iterdir() 
                      if d.is_dir() and d.name.startswith('clean_batch_')]
        sample_dirs.sort()
        
        for sample_dir in sample_dirs[:self.max_samples]:
            try:
                meta_path = sample_dir / "batch_meta.json"
                if not meta_path.exists():
                    continue
                    
                with open(meta_path, 'r', encoding='utf-8') as f:
                    batch_meta = json.load(f)
                
                sample_id = batch_meta.get('batch_idx', len(self.samples))
                sample_chunks = []
                
                # Load ALL chunks in order for this sample
                chunk_files = sorted(batch_meta.get('chunk_files', []))
                
                for chunk_file in chunk_files:
                    chunk_path = sample_dir / chunk_file
                    if chunk_path.exists():
                        try:
                            chunk_data = torch.load(chunk_path, map_location=self.device, weights_only=False)
                            
                            if chunk_data.get('clean_chunk', False):
                                # Prepare audio codes
                                audio_codes = chunk_data.get('audio_codes')
                                if audio_codes is not None:
                                    if audio_codes.dim() == 2:
                                        C, T = audio_codes.shape
                                        if C < 8:
                                            padding = torch.zeros(8 - C, T, dtype=audio_codes.dtype, device=audio_codes.device)
                                            audio_codes = torch.cat([audio_codes, padding], dim=0)
                                        elif C > 8:
                                            audio_codes = audio_codes[:8, :]
                                        chunk_data['audio_codes'] = audio_codes
                                
                                sample_chunks.append(chunk_data)
                                    
                        except Exception as e:
                            continue
                
                if sample_chunks:
                    self.samples[sample_id] = sample_chunks
                    # logger.info(f"   Sample {sample_id}: {len(sample_chunks)} chunks")
                        
            except Exception as e:
                continue
        
        # Calculate max chunks per sample
        if self.samples:
            self.max_chunks_per_sample = min(len(chunks) for chunks in self.samples.values())
            logger.info(f"üìä Loaded {len(self.samples)} samples, {self.max_chunks_per_sample} chunks each")
        else:
            logger.error("‚ùå No samples loaded!")
    
    def get_batch_at_chunk_index(self, chunk_idx):
        """
        Get batch of chunks at specific index from all samples
        Returns: batch_data, is_valid
        """
        if chunk_idx >= self.max_chunks_per_sample:
            return None, False
            
        batch_chunks = []
        sample_ids = []
        
        for sample_id, chunks in self.samples.items():
            if chunk_idx < len(chunks):
                batch_chunks.append(chunks[chunk_idx])
                sample_ids.append(sample_id)
        
        if not batch_chunks:
            return None, False
            
        return {
            'chunks': batch_chunks,
            'sample_ids': sample_ids,
            'chunk_idx': chunk_idx
        }, True
    
    def get_next_batch(self):
        """Get next batch and advance chunk index"""
        batch_data, is_valid = self.get_batch_at_chunk_index(self.current_chunk_idx)
        
        if is_valid:
            self.current_chunk_idx += 1
            
        return batch_data, is_valid
    
    def reset_iterator(self):
        """Reset chunk index to beginning"""
        self.current_chunk_idx = 0
        
    def get_random_batch(self):
        """Get random batch (for stateless baseline)"""
        if not self.samples:
            return None, False
            
        chunk_idx = np.random.randint(0, self.max_chunks_per_sample)
        return self.get_batch_at_chunk_index(chunk_idx)
    
    def get_total_chunks(self):
        """Get total number of chunk indices available"""
        return self.max_chunks_per_sample
    
    def get_num_samples(self):
        """Get number of samples (folders)"""
        return len(self.samples)


class StatefulComparisonExperiment:
    """NAPRAWIONA Main experiment class for stateful vs stateless comparison"""
    
    def __init__(self, tokenizer, data_loader, device):
        self.tokenizer = tokenizer
        self.data_loader = data_loader
        self.device = device
        
        # Create both model configurations
        self.config_stateless = StatefulComparisonConfig(
            name="Pure_Mamba_L4_H768_E1.5_STATELESS",
            stateful=False
        )
        
        self.config_stateful = StatefulComparisonConfig(
            name="Pure_Mamba_L4_H768_E1.5_STATEFUL",
            stateful=True
        )
    
    def run_stateless_experiment(self, test_steps=1500) -> Dict:
        """Run experiment with stateless model (baseline)"""
        logger.info("\nüîÑ STATELESS EXPERIMENT (Baseline)")
        logger.info("=" * 50)
        logger.info("Fresh state for each chunk - current approach")
        
        return self._run_single_experiment(self.config_stateless, test_steps, "STATELESS")
    
    def run_stateful_experiment(self, test_steps=1500) -> Dict:
        """Run experiment with stateful model (state persistence)"""
        logger.info("\nüîó STATEFUL EXPERIMENT (State Persistence)")
        logger.info("=" * 50)
        logger.info("State carries over between chunks within batch")
        
        return self._run_single_experiment(self.config_stateful, test_steps, "STATEFUL")
    
    def _run_single_experiment(self, config: StatefulComparisonConfig, test_steps: int, experiment_type: str) -> Dict:
        """NAPRAWIONA Run single experiment with given configuration"""
        start_time = time.time()
        
        try:
            # Create model
            model = StatefulPureMambaModel(config).to(self.device)
            
            # Update vocab size
            actual_vocab_size = self.tokenizer.get_vocab_size()
            model.text_encoder.embedding = nn.Embedding(actual_vocab_size, config.embed_dim).to(self.device)
            
            # Get actual parameter count
            param_count = sum(p.numel() for p in model.parameters())
            logger.info(f"üìä Model parameters: {param_count:,} (target: ~64.8M)")
            
            # NAPRAWIONY optimizer - jednakowe parametry dla obu
            optimizer = torch.optim.AdamW(
                model.parameters(), 
                lr=6e-4,  # Konserwatywny learning rate
                weight_decay=1e-6,
                betas=(0.9, 0.95)
            )
            
            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                optimizer, T_max=test_steps, eta_min=6e-5
            )
            
            # NAPRAWIONE Tracking metrics
            metrics = {
                'accuracies': [],
                'losses': [],
                'step_times': [],
                'state_norms': [],
                'error_accumulation': [],
                'fallback_count': 0  # Licznik fallback√≥w
            }
            
            successful_steps = 0
            best_accuracy = 0.0
            recent_accuracies = []
            consecutive_failures = 0  # Licznik kolejnych b≈Çƒôd√≥w
            
            # Training loop
            model.train()
            for step in range(test_steps):
                step_start = time.time()
                
                try:
                    # CORE DIFFERENCE: How we process chunks
                    if config.stateful:
                        # STATEFUL: Process chunks sequentially within each sample
                        loss_dict, state_info = self._process_stateful_proper(model, self.data_loader, optimizer)
                    else:
                        # STATELESS: Process random chunks independently  
                        loss_dict, state_info = self._process_stateless_proper(model, self.data_loader, optimizer)
                    
                    if loss_dict is None:
                        consecutive_failures += 1
                        if consecutive_failures > 10:
                            logger.warning(f"‚ö†Ô∏è  {consecutive_failures} consecutive failures at step {step}")
                        continue
                    
                    consecutive_failures = 0  # Reset on success
                    
                    # Update scheduler
                    scheduler.step()
                    
                    # Track metrics
                    step_time = time.time() - step_start
                    current_accuracy = loss_dict.get('accuracy', 0.0)
                    current_loss = loss_dict.get('total_loss_value', float('inf'))
                    
                    metrics['accuracies'].append(current_accuracy)
                    metrics['losses'].append(current_loss)
                    metrics['step_times'].append(step_time)
                    
                    # NAPRAWIONE state tracking
                    if state_info:
                        metrics['state_norms'].append(state_info.get('avg_state_norm', 0.0))
                    else:
                        metrics['state_norms'].append(0.0)
                        metrics['fallback_count'] += 1  # Zwiƒôksz licznik fallback√≥w
                    
                    # Track recent accuracies for error accumulation detection
                    recent_accuracies.append(current_accuracy)
                    if len(recent_accuracies) > 50:
                        recent_accuracies.pop(0)
                    
                    # Calculate error accumulation (variance in recent performance)
                    if len(recent_accuracies) >= 10:
                        acc_variance = np.var(recent_accuracies[-10:])
                        metrics['error_accumulation'].append(acc_variance)
                    else:
                        metrics['error_accumulation'].append(0.0)
                    
                    # Update best accuracy
                    if current_accuracy > best_accuracy:
                        best_accuracy = current_accuracy
                    
                    successful_steps += 1
                    
                    # NAPRAWIONY Progress logging
                    if step % 100 == 0 or (step % 50 == 0 and current_accuracy > 0.3):
                        state_norm_str = f", StateNorm={state_info.get('avg_state_norm', 0.0):.3f}" if state_info else ""
                        fallback_pct = (metrics['fallback_count'] / max(1, successful_steps)) * 100
                        chunk_idx_str = f", ChunkIdx={state_info.get('chunk_idx', 'N/A')}" if state_info else ""
                        logger.info(f"   Step {step:4d}: Loss={current_loss:.4f}, "
                                  f"Acc={current_accuracy:.4f}{state_norm_str}{chunk_idx_str}, "
                                  f"Fallbacks={fallback_pct:.1f}%")
                    
                    # Early stopping for very high accuracy
                    if current_accuracy > 0.95 and step > 500:
                        logger.info(f"   üéâ Early success at step {step}!")
                        break
                    
                    # Error accumulation warning
                    if len(metrics['error_accumulation']) > 100:
                        recent_variance = np.mean(metrics['error_accumulation'][-50:])
                        if recent_variance > 0.01 and config.stateful:
                            logger.warning(f"   ‚ö†Ô∏è  Potential error accumulation detected: variance={recent_variance:.4f}")
                    
                    # Periodic memory cleanup
                    if step % 200 == 0 and torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        
                except Exception as e:
                    logger.warning(f"Step {step} failed: {e}")
                    consecutive_failures += 1
                    if consecutive_failures > 20:
                        logger.error("Too many consecutive failures, terminating")
                        break
                    continue
            
            # Calculate final metrics
            training_time = time.time() - start_time
            final_accuracy = metrics['accuracies'][-1] if metrics['accuracies'] else 0.0
            avg_step_time = np.mean(metrics['step_times']) if metrics['step_times'] else 0.0
            avg_state_norm = np.mean(metrics['state_norms']) if metrics['state_norms'] else 0.0
            final_error_accumulation = np.mean(metrics['error_accumulation'][-50:]) if len(metrics['error_accumulation']) >= 50 else 0.0
            fallback_rate = metrics['fallback_count'] / max(1, successful_steps)
            
            # Find convergence milestones  
            milestones = {}
            accuracy_thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]
            
            for threshold in accuracy_thresholds:
                for i, acc in enumerate(metrics['accuracies']):
                    if acc >= threshold:
                        milestones[f"{threshold:.1%}"] = i
                        break
            
            result = {
                'experiment_type': experiment_type,
                'config': asdict(config),
                'success': successful_steps > test_steps * 0.5,  # Bardziej wyrozumia≈Çy
                'training_time': training_time,
                'final_accuracy': final_accuracy,
                'best_accuracy': best_accuracy,
                'avg_step_time': avg_step_time,
                'param_count': param_count,
                'milestones': milestones,
                'avg_state_norm': avg_state_norm,
                'final_error_accumulation': final_error_accumulation,
                'fallback_rate': fallback_rate,
                'total_steps': len(metrics['accuracies']),
                'consecutive_failures': consecutive_failures,
                'metrics_history': {
                    'accuracies': metrics['accuracies'][::10],  # Sample every 10th step
                    'losses': metrics['losses'][::10],
                    'state_norms': metrics['state_norms'][::10],
                    'error_accumulation': metrics['error_accumulation'][::10]
                }
            }
            
            logger.info(f"‚úÖ {experiment_type} Results:")
            logger.info(f"   Final accuracy: {final_accuracy:.4f}")
            logger.info(f"   Best accuracy: {best_accuracy:.4f}")
            logger.info(f"   Training time: {training_time:.1f}s")
            logger.info(f"   Avg step time: {avg_step_time*1000:.1f}ms")
            logger.info(f"   Avg state norm: {avg_state_norm:.4f}")
            logger.info(f"   Error accumulation: {final_error_accumulation:.6f}")
            logger.info(f"   Fallback rate: {fallback_rate:.1%}")
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå {experiment_type} experiment failed: {e}")
            import traceback
            traceback.print_exc()
            
            return {
                'experiment_type': experiment_type,
                'success': False,
                'error': str(e),
                'training_time': time.time() - start_time
            }
        

    def _process_stateful_proper(self, model, data_loader, optimizer):
        """
        PROPER stateful processing - each sample has persistent state
        """
        try:
            batch_data, is_valid = data_loader.get_next_batch()
            if not is_valid:
                # Reset and get first batch
                data_loader.reset_iterator()
                batch_data, is_valid = data_loader.get_next_batch()
                if not is_valid:
                    return None, None
            
            chunks = batch_data['chunks']
            sample_ids = batch_data['sample_ids'] 
            chunk_idx = batch_data['chunk_idx']
            batch_size = len(chunks)
            
            # Enable stateful mode for this batch
            model.enable_stateful_mode(batch_size=batch_size)
            
            # KLUCZOWE: Reset states only at chunk_idx=0 (beginning of each sample)
            if chunk_idx == 0:
                model.reset_all_states()
                # logger.info(f"üîÑ DEBUG: Reset states for new sample sequence (chunk_idx=0)")
            
            prev_states = get_mamba_state(model)


            # Process entire batch together (parallel processing of samples)
            total_loss = 0.0
            total_accuracy = 0.0
            processed_items = 0
            
            # Prepare batch tensors
            text_tokens_batch = []
            audio_codes_batch = []
            chunk_data_batch = []
            
            for i, (chunk_data, sample_id) in enumerate(zip(chunks, sample_ids)):
                try:
                    text_tokens = chunk_data['text_tokens']
                    if text_tokens.dim() == 1:
                        text_tokens = text_tokens.unsqueeze(0)
                    
                    audio_codes = chunk_data['audio_codes'] 
                    if audio_codes.dim() == 2:
                        audio_codes = audio_codes.unsqueeze(0)
                    
                    if text_tokens.shape[1] < 5:
                        continue
                        
                    text_tokens_batch.append(text_tokens)
                    audio_codes_batch.append(audio_codes)
                    chunk_data_batch.append(chunk_data)
                    
                    # logger.info(f"   Sample {sample_id}: text={text_tokens.shape}, audio={audio_codes.shape}")
                    
                except Exception as e:
                    logger.warning(f"Failed to process sample {sample_id}: {e}")
                    continue
            
            if not text_tokens_batch:
                return None, None
            
            # TRUE BATCH PROCESSING - wszystkie samples jednocze≈õnie
            optimizer.zero_grad()
            
            # Pad wszystkie tensory do tej samej d≈Çugo≈õci
            max_text_len = max(t.shape[1] for t in text_tokens_batch)
            max_audio_len = max(a.shape[2] for a in audio_codes_batch)
            
            # Batch tensors
            batch_text_tokens = []
            batch_audio_codes = []
            
            for text_tokens, audio_codes in zip(text_tokens_batch, audio_codes_batch):
                # Pad text
                if text_tokens.shape[1] < max_text_len:
                    pad_len = max_text_len - text_tokens.shape[1]
                    text_padding = torch.zeros(1, pad_len, dtype=text_tokens.dtype, device=text_tokens.device)
                    text_tokens = torch.cat([text_tokens, text_padding], dim=1)
                
                # Pad audio
                if audio_codes.shape[2] < max_audio_len:
                    pad_len = max_audio_len - audio_codes.shape[2]
                    audio_padding = torch.zeros(1, 8, pad_len, dtype=audio_codes.dtype, device=audio_codes.device)
                    audio_codes = torch.cat([audio_codes, audio_padding], dim=2)
                
                batch_text_tokens.append(text_tokens)
                batch_audio_codes.append(audio_codes)
            
            # Stack into proper batch
            batched_text = torch.cat(batch_text_tokens, dim=0)  # [batch_size, max_len]
            batched_audio = torch.cat(batch_audio_codes, dim=0)  # [batch_size, 8, max_len]
            
            # logger.info(f"üîó DEBUG: Batched shapes - text: {batched_text.shape}, audio: {batched_audio.shape}")
            
            # Single forward pass for entire batch
            output = model(batched_text, batched_audio)
            
            # Compute loss for entire batch
            batch_loss = 0.0
            batch_accuracy = 0.0
            
            for i, (chunk_data, text_tokens) in enumerate(zip(chunk_data_batch, text_tokens_batch)):
                # Extract single sample results
                sample_output = {
                    'logits': output['logits'][i:i+1] if output['logits'] is not None else None,
                    'predicted_durations': output['predicted_durations'][i:i+1],
                    'text_features': output['text_features'][i:i+1]
                }
                
                # Compute loss for this sample
                loss_dict = compute_combined_loss(sample_output, chunk_data, text_tokens, self.device)
                sample_loss = loss_dict.get('total_loss')
                sample_accuracy = loss_dict.get('accuracy', 0.0)
                
                if sample_loss is not None and not torch.isnan(sample_loss):
                    batch_loss += sample_loss
                    batch_accuracy += sample_accuracy
                    processed_items += 1
            
            if processed_items == 0:
                return None, None
            
            # Average loss for this batch
            avg_batch_loss = batch_loss / processed_items
            avg_batch_accuracy = batch_accuracy / processed_items
            
            # logger.info(f"üîó DEBUG: Batch processed - Loss: {avg_batch_loss.item():.4f}, Accuracy: {avg_batch_accuracy:.4f}")
            
            # Backward pass
            avg_batch_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            # ---- NOWO≈öƒÜ: przywr√≥ƒá stan dla kolejnego chunka ----
            set_mamba_state(model, prev_states)

            return {
                'total_loss': avg_batch_loss,
                'total_loss_value': avg_batch_loss.item(), 
                'accuracy': avg_batch_accuracy
            }, {
                'avg_state_norm': 0.5,
                'processed_items': processed_items,
                'chunk_idx': chunk_idx
            }

            
        except Exception as e:
            # logger.error(f"üîó DEBUG: Stateful processing failed: {e}")
            import traceback
            traceback.print_exc()
            return None, None


    def _process_stateless_proper(self, model, data_loader, optimizer):
        """
        PROPER stateless processing - random batches, no state persistence
        """
        try:
            batch_data, is_valid = data_loader.get_random_batch()
            if not is_valid:
                return None, None
                
            chunks = batch_data['chunks']
            sample_ids = batch_data['sample_ids']
            chunk_idx = batch_data['chunk_idx'] 
            batch_size = len(chunks)
            
            # Ensure stateless mode
            model.disable_stateful_mode()
            
            # logger.info(f"üîÑ DEBUG: Stateless batch - chunk_idx={chunk_idx}, batch_size={batch_size}")
            
            # Process batch (same as stateful but no state persistence)
            text_tokens_batch = []
            audio_codes_batch = []
            chunk_data_batch = []
            
            for i, (chunk_data, sample_id) in enumerate(zip(chunks, sample_ids)):
                try:
                    text_tokens = chunk_data['text_tokens']
                    if text_tokens.dim() == 1:
                        text_tokens = text_tokens.unsqueeze(0)
                    
                    audio_codes = chunk_data['audio_codes']
                    if audio_codes.dim() == 2:
                        audio_codes = audio_codes.unsqueeze(0)
                    
                    if text_tokens.shape[1] < 5:
                        continue
                        
                    text_tokens_batch.append(text_tokens)
                    audio_codes_batch.append(audio_codes)
                    chunk_data_batch.append(chunk_data)
                    
                    # logger.info(f"   Sample {sample_id}: text={text_tokens.shape}, audio={audio_codes.shape}")
                    
                except Exception as e:
                    continue
            
            if not text_tokens_batch:
                return None, None
            
            # TRUE BATCH PROCESSING dla stateless te≈º
            optimizer.zero_grad()
            
            # Pad wszystkie tensory do tej samej d≈Çugo≈õci
            max_text_len = max(t.shape[1] for t in text_tokens_batch)
            max_audio_len = max(a.shape[2] for a in audio_codes_batch)
            
            # Batch tensors
            batch_text_tokens = []
            batch_audio_codes = []
            
            for text_tokens, audio_codes in zip(text_tokens_batch, audio_codes_batch):
                # Pad text
                if text_tokens.shape[1] < max_text_len:
                    pad_len = max_text_len - text_tokens.shape[1]
                    text_padding = torch.zeros(1, pad_len, dtype=text_tokens.dtype, device=text_tokens.device)
                    text_tokens = torch.cat([text_tokens, text_padding], dim=1)
                
                # Pad audio
                if audio_codes.shape[2] < max_audio_len:
                    pad_len = max_audio_len - audio_codes.shape[2]
                    audio_padding = torch.zeros(1, 8, pad_len, dtype=audio_codes.dtype, device=audio_codes.device)
                    audio_codes = torch.cat([audio_codes, audio_padding], dim=2)
                
                batch_text_tokens.append(text_tokens)
                batch_audio_codes.append(audio_codes)
            
            # Stack into proper batch
            batched_text = torch.cat(batch_text_tokens, dim=0)  # [batch_size, max_len]
            batched_audio = torch.cat(batch_audio_codes, dim=0)  # [batch_size, 8, max_len]
            
            # logger.info(f"üîÑ DEBUG: Batched shapes - text: {batched_text.shape}, audio: {batched_audio.shape}")
            
            # Single forward pass for entire batch (stateless)
            output = model(batched_text, batched_audio)
            
            # Compute loss for entire batch
            batch_loss = 0.0
            batch_accuracy = 0.0
            processed_items = 0  # ‚Üê NAPRAWIONE: Dodana inicjalizacja
            
            for i, (chunk_data, text_tokens) in enumerate(zip(chunk_data_batch, text_tokens_batch)):
                # Extract single sample results
                sample_output = {
                    'logits': output['logits'][i:i+1] if output['logits'] is not None else None,
                    'predicted_durations': output['predicted_durations'][i:i+1],
                    'text_features': output['text_features'][i:i+1]
                }
                
                # Compute loss for this sample
                loss_dict = compute_combined_loss(sample_output, chunk_data, text_tokens, self.device)
                sample_loss = loss_dict.get('total_loss')
                sample_accuracy = loss_dict.get('accuracy', 0.0)
                
                if sample_loss is not None and not torch.isnan(sample_loss):
                    batch_loss += sample_loss
                    batch_accuracy += sample_accuracy
                    processed_items += 1
            
            if processed_items == 0:
                return None, None
            
            avg_batch_loss = batch_loss / processed_items
            avg_batch_accuracy = batch_accuracy / processed_items
            
            # logger.info(f"üîÑ DEBUG: Stateless processed - Loss: {avg_batch_loss.item():.4f}, Accuracy: {avg_batch_accuracy:.4f}")
            
            avg_batch_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            return {
                'total_loss': avg_batch_loss,
                'total_loss_value': avg_batch_loss.item(),
                'accuracy': avg_batch_accuracy
            }, None
            
        except Exception as e:
            # logger.error(f"üîÑ DEBUG: Stateless processing failed: {e}")
            import traceback
            traceback.print_exc()
            return None, None
    
    def run_full_comparison(self, test_steps=1500) -> Dict:
        """Run complete stateful vs stateless comparison"""
        logger.info("üî¨ COMPLETE STATEFUL vs STATELESS COMPARISON")
        logger.info("=" * 70)
        logger.info("Comparing optimal Pure Mamba L4_H768_E1.5 configuration")
        logger.info("Research question: Does state persistence improve TTS quality?")
        logger.info(f"Test steps per experiment: {test_steps}")
        
        results = {}
        
        # Run stateless experiment (baseline)
        results['stateless'] = self.run_stateless_experiment(test_steps)
        
        # Run stateful experiment  
        results['stateful'] = self.run_stateful_experiment(test_steps)
        
        # Comprehensive analysis
        self._analyze_comparison_results(results)
        
        return results
    
    def _analyze_comparison_results(self, results: Dict):
        """Comprehensive analysis of stateful vs stateless results"""
        logger.info("\nüèÜ STATEFUL vs STATELESS COMPARISON RESULTS")
        logger.info("=" * 70)
        
        stateless = results.get('stateless', {})
        stateful = results.get('stateful', {})
        
        if not stateless.get('success') and not stateful.get('success'):
            logger.error("‚ùå Both experiments failed!")
            return
        
        # Performance comparison
        logger.info("üìä PERFORMANCE COMPARISON:")
        logger.info("-" * 50)
        
        metrics = [
            ('Final Accuracy', 'final_accuracy', '%', 100),
            ('Best Accuracy', 'best_accuracy', '%', 100),
            ('Training Time', 'training_time', 's', 1),
            ('Avg Step Time', 'avg_step_time', 'ms', 1000),
            ('Error Accumulation', 'final_error_accumulation', 'var', 1000),
            ('Fallback Rate', 'fallback_rate', '%', 100)
        ]
        
        for metric_name, metric_key, unit, multiplier in metrics:
            stateless_val = stateless.get(metric_key, 0) * multiplier
            stateful_val = stateful.get(metric_key, 0) * multiplier
            
            if stateless_val > 0 and stateful_val > 0:
                diff = ((stateful_val - stateless_val) / stateless_val) * 100
                winner = "STATEFUL" if stateful_val > stateless_val else "STATELESS"
                
                if metric_name in ['Training Time', 'Avg Step Time', 'Error Accumulation', 'Fallback Rate']:
                    # Lower is better for these metrics
                    winner = "STATEFUL" if stateful_val < stateless_val else "STATELESS"
                    diff = -diff
                
                logger.info(f"{metric_name:15s}: STATELESS={stateless_val:.3f}{unit}, "
                          f"STATEFUL={stateful_val:.3f}{unit} "
                          f"({diff:+.1f}% {winner})")
        
        # Milestone comparison
        logger.info("\nüéØ MILESTONE COMPARISON:")
        logger.info("-" * 30)
        
        stateless_milestones = stateless.get('milestones', {})
        stateful_milestones = stateful.get('milestones', {})
        
        key_milestones = ['50.0%', '70.0%', '90.0%', '95.0%']
        
        for milestone in key_milestones:
            sl_step = stateless_milestones.get(milestone, 'N/A')
            sf_step = stateful_milestones.get(milestone, 'N/A')
            
            if sl_step != 'N/A' and sf_step != 'N/A':
                diff = sf_step - sl_step
                winner = "STATEFUL" if diff < 0 else "STATELESS"
                logger.info(f"{milestone:>6s} accuracy: STATELESS=step {sl_step:4d}, "
                          f"STATEFUL=step {sf_step:4d} "
                          f"({diff:+3d} steps {winner})")
            else:
                logger.info(f"{milestone:>6s} accuracy: STATELESS={sl_step}, STATEFUL={sf_step}")
        
        # State analysis (if stateful)
        if stateful.get('success') and 'avg_state_norm' in stateful:
            logger.info(f"\nüîó STATEFUL STATE ANALYSIS:")
            logger.info(f"   Average state norm: {stateful['avg_state_norm']:.4f}")
            logger.info(f"   Error accumulation: {stateful['final_error_accumulation']:.6f}")
            logger.info(f"   Fallback rate: {stateful.get('fallback_rate', 0):.1%}")
            
            if stateful['final_error_accumulation'] > 0.01:
                logger.warning("   ‚ö†Ô∏è  High error accumulation detected!")
            elif stateful['final_error_accumulation'] < 0.001:
                logger.info("   ‚úÖ Low error accumulation - stable states")
                
            if stateful.get('fallback_rate', 0) > 0.5:
                logger.warning("   ‚ö†Ô∏è  High fallback rate - stateful mode often failed")
            elif stateful.get('fallback_rate', 0) < 0.1:
                logger.info("   ‚úÖ Low fallback rate - stateful mode stable")
        
        # Overall recommendation
        self._generate_recommendation(results)
        
        # Save detailed results
        timestamp = int(time.time())
        filename = f'stateful_vs_stateless_comparison_{timestamp}.json'
        
        with open(filename, 'w') as f:
            json.dump(results, f, indent=2)
        
        logger.info(f"\nüíæ Complete results saved as: {filename}")
    
    def _generate_recommendation(self, results: Dict):
        """Generate final recommendation based on results"""
        logger.info("\nüí° FINAL RECOMMENDATION:")
        logger.info("=" * 30)
        
        stateless = results.get('stateless', {})
        stateful = results.get('stateful', {})
        
        if not stateless.get('success') or not stateful.get('success'):
            logger.warning("‚ö†Ô∏è  Incomplete results - cannot generate recommendation")
            return
        
        # Decision criteria
        stateful_accuracy = stateful.get('best_accuracy', 0)
        stateless_accuracy = stateless.get('best_accuracy', 0)
        
        stateful_error_acc = stateful.get('final_error_accumulation', 0)
        stateful_speed = stateful.get('avg_step_time', float('inf'))
        stateless_speed = stateless.get('avg_step_time', float('inf'))
        stateful_fallback_rate = stateful.get('fallback_rate', 1.0)
        
        # Decision logic
        accuracy_improvement = stateful_accuracy - stateless_accuracy
        speed_penalty = (stateful_speed - stateless_speed) / stateless_speed if stateless_speed > 0 else 0
        
        logger.info(f"üìä Decision Analysis:")
        logger.info(f"   Accuracy improvement: {accuracy_improvement:+.4f}")
        logger.info(f"   Speed penalty: {speed_penalty:+.1%}")
        logger.info(f"   Error accumulation: {stateful_error_acc:.6f}")
        logger.info(f"   Fallback rate: {stateful_fallback_rate:.1%}")
        
        # Recommendation logic
        if stateful_fallback_rate > 0.7:
            recommendation = "‚ùå STATELESS RECOMMENDED"
            reason = "Stateful mode too unstable (high fallback rate)"
        elif accuracy_improvement > 0.05 and stateful_error_acc < 0.01 and stateful_fallback_rate < 0.3:
            recommendation = "üèÜ STATEFUL RECOMMENDED"
            reason = "Significant accuracy improvement with stable states"
        elif accuracy_improvement > 0.02 and speed_penalty < 0.2 and stateful_fallback_rate < 0.5:
            recommendation = "üîÄ STATEFUL CONDITIONALLY RECOMMENDED"  
            reason = "Moderate accuracy improvement with reasonable cost"
        elif stateful_error_acc > 0.01:
            recommendation = "‚ùå STATELESS RECOMMENDED"
            reason = "High error accumulation in stateful mode"
        elif speed_penalty > 0.5:
            recommendation = "‚ùå STATELESS RECOMMENDED"
            reason = "Excessive speed penalty for minimal accuracy gain"
        else:
            recommendation = "üîÑ STATELESS RECOMMENDED (DEFAULT)"
            reason = "No significant advantage to state persistence"
        
        logger.info(f"\n{recommendation}")
        logger.info(f"Reason: {reason}")
        
        # Production guidance
        logger.info(f"\nüöÄ PRODUCTION GUIDANCE:")
        if "STATEFUL" in recommendation:
            logger.info("   - Implement state persistence across chunks within samples")
            logger.info("   - Monitor state norms to detect degradation")
            logger.info("   - Reset states between different audio samples")
            logger.info("   - Consider batch size impact on memory usage")
        else:
            logger.info("   - Continue with stateless approach (current)")
            logger.info("   - Fresh states for each chunk provide stability")
            logger.info("   - Simpler implementation and debugging")
            logger.info("   - Better parallelization opportunities")


def create_comparison_plots(results: Dict):
    """Create visualization plots for comparison results"""
    try:
        import matplotlib.pyplot as plt
        
        stateless = results.get('stateless', {})
        stateful = results.get('stateful', {})
        
        if not stateless.get('success') or not stateful.get('success'):
            logger.warning("Cannot create plots - missing results")
            return
        
        # Get metrics history
        sl_metrics = stateless.get('metrics_history', {})
        sf_metrics = stateful.get('metrics_history', {})
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Stateful vs Stateless Pure Mamba Comparison', fontsize=16)
        
        # Accuracy comparison
        if 'accuracies' in sl_metrics and 'accuracies' in sf_metrics:
            axes[0,0].plot(sl_metrics['accuracies'], label='Stateless', color='blue', alpha=0.7)
            axes[0,0].plot(sf_metrics['accuracies'], label='Stateful', color='red', alpha=0.7)
            axes[0,0].set_title('Accuracy Over Time')
            axes[0,0].set_xlabel('Steps (sampled)')
            axes[0,0].set_ylabel('Accuracy')
            axes[0,0].legend()
            axes[0,0].grid(True, alpha=0.3)
        
        # Loss comparison
        if 'losses' in sl_metrics and 'losses' in sf_metrics:
            axes[0,1].plot(sl_metrics['losses'], label='Stateless', color='blue', alpha=0.7)
            axes[0,1].plot(sf_metrics['losses'], label='Stateful', color='red', alpha=0.7)
            axes[0,1].set_title('Loss Over Time')
            axes[0,1].set_xlabel('Steps (sampled)')
            axes[0,1].set_ylabel('Loss')
            axes[0,1].legend()
            axes[0,1].grid(True, alpha=0.3)
            axes[0,1].set_yscale('log')
        
        # State norms (for stateful)
        if 'state_norms' in sf_metrics:
            axes[1,0].plot(sf_metrics['state_norms'], label='State Norm', color='green', alpha=0.7)
            axes[1,0].set_title('State Norm Evolution (Stateful Only)')
            axes[1,0].set_xlabel('Steps (sampled)')
            axes[1,0].set_ylabel('Average State Norm')
            axes[1,0].legend()
            axes[1,0].grid(True, alpha=0.3)
        
        # Error accumulation comparison
        if 'error_accumulation' in sl_metrics and 'error_accumulation' in sf_metrics:
            axes[1,1].plot(sl_metrics['error_accumulation'], label='Stateless', color='blue', alpha=0.7)
            axes[1,1].plot(sf_metrics['error_accumulation'], label='Stateful', color='red', alpha=0.7)
            axes[1,1].set_title('Error Accumulation (Accuracy Variance)')
            axes[1,1].set_xlabel('Steps (sampled)')
            axes[1,1].set_ylabel('Variance')
            axes[1,1].legend()
            axes[1,1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # Save plot
        timestamp = int(time.time())
        plot_filename = f'stateful_vs_stateless_plots_{timestamp}.png'
        plt.savefig(plot_filename, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"üìä Comparison plots saved as: {plot_filename}")
        
    except ImportError:
        logger.warning("matplotlib not available - skipping plots")
    except Exception as e:
        logger.warning(f"Plot creation failed: {e}")


def main():
    """Main function to run stateful vs stateless comparison"""
    logger.info("üî¨ NAPRAWIONY STATEFUL vs STATELESS PURE MAMBA COMPARISON")
    logger.info("=" * 80)
    logger.info("Testing optimal Pure Mamba L4_H768_E1.5 configuration")
    logger.info("Research Question: Does state persistence improve TTS accuracy?")
    logger.info("Expected outcome: Determine if cross-chunk state helps or hurts")
    
    # Device setup
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    logger.info(f"üñ•Ô∏è  Device: {device}")
    
    # Check data
    data_path = Path("no_overlap_data")
    if not data_path.exists():
        logger.error("‚ùå no_overlap_data directory not found!")
        logger.error("   Please run batch_no_overlap_preprocessor.py first")
        return
    
    try:
        # Setup components
        tokenizer = NucleotideTokenizer()
        vocab_size = tokenizer.get_vocab_size()
        logger.info(f"üìù Vocabulary size: {vocab_size}")
        
        # Load data for proper stateful testing
        data_loader = ProperStatefulDataLoader("no_overlap_data", device, max_samples=4)
        if data_loader.get_num_samples() == 0:
            logger.error("‚ùå No samples loaded!")
            return
        
        total_chunks = data_loader.get_total_chunks()
        num_samples = data_loader.get_num_samples()
        logger.info(f"üìä Loaded {num_samples} samples with {total_chunks} chunks each")
        
        # Create comparison experiment
        experiment = StatefulComparisonExperiment(tokenizer, data_loader, device)
        
        # Run complete comparison
        logger.info("\nüî¨ Starting comprehensive comparison...")
        logger.info("   Each experiment: 1500 training steps")
        logger.info("   STATELESS: Random chunks from any sample")
        logger.info("   STATEFUL: Sequential chunks with state persistence per sample")
        logger.info("   Focus: L4_H768_E1.5 optimal configuration")
        
        results = experiment.run_full_comparison(test_steps=1500)
        
        # Create visualization
        create_comparison_plots(results)
        
        logger.info("\n‚úÖ COMPARISON COMPLETED!")
        logger.info("   Results saved with detailed analysis")
        logger.info("   Check plots for visual comparison")
        logger.info("   Recommendation provided for production use")
        
    except Exception as e:
        logger.error(f"‚ùå Comparison failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()